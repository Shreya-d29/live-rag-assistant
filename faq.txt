Q: What is RAG?
A: RAG (Retrieval-Augmented Generation) is a technique that combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them as context for generating accurate answers.

Q: How does the live update feature work?
A: The system monitors the FAQ file for changes. When detected, it automatically re-processes the content, creates new embeddings, and updates the vector store without requiring an application restart.

Q: What is FAISS?
A: FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It's used to quickly find the most relevant FAQ entries for a given query.

Q: Which LLM models are supported?
A: The system supports both Ollama (local models like Llama 2, Mistral) and OpenAI GPT models (GPT-3.5, GPT-4). You can configure which model to use in the settings.

Q: What are embeddings?
A: Embeddings are numerical vector representations of text that capture semantic meaning. Similar texts have similar embeddings, enabling efficient similarity search.

Q: How accurate are the answers?
A: Answer accuracy depends on the quality of the FAQ content and the LLM model used. The RAG approach ensures answers are grounded in your knowledge base, reducing hallucinations.

Q: Can I use multiple documents?
A: Yes, the system can be extended to support multiple file formats including TXT, PDF, CSV, and DOCX files.

Q: What is the response time?
A: Typical response time is 1-3 seconds, depending on the LLM model, query complexity, and vector store size.